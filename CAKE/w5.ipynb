{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "base_info = pd.read_csv('EnergyContest/BSinfo.csv')\n",
    "cell_data = pd.read_csv('EnergyContest/CLdata.csv', parse_dates=['Time'])\n",
    "energy_data = pd.read_csv('EnergyContest/ECdata.csv', parse_dates=['Time'])\n",
    "power_consumption_prediction = pd.read_csv('EnergyContest/power_consumption_prediction.csv', parse_dates=['Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine CLdata and BSinfo\n",
    "Total = pd.merge(base_info,cell_data, on=['BS', 'CellName'], how='inner')\n",
    "Total_dummies = pd.get_dummies(Total, columns=['Mode','RUType','Antennas','Bandwidth','Frequency']) # one-hot encoding\n",
    "Total_pivot = pd.pivot_table(Total_dummies, values=[col for col in Total_dummies.columns if (col != 'BS') and (col != 'Time') ], index=['Time', 'BS'], columns='CellName', fill_value=0)\n",
    "Total_pivot.reset_index(inplace=True)\n",
    "Total_pivot.columns = [f'{col[0]}_{col[1]}' if col[1] else col[0] for col in Total_pivot.columns]\n",
    "\n",
    "# Add feature : mean of load, sum of load, mean of ESMode, sum of ESMode\n",
    "load_columns = [col for col in Total_pivot.columns if col.startswith('load')]\n",
    "Total_pivot['load_avg'] = Total_pivot[load_columns].mean(axis=1)\n",
    "Total_pivot['load_sum'] = Total_pivot[load_columns].sum(axis=1)\n",
    "ESMode_columns = [col for col in Total_pivot.columns if col.startswith('ESMode')]\n",
    "Total_pivot['ESMode_avg'] = Total_pivot[ESMode_columns].mean(axis=1)\n",
    "Total_pivot['ESMode_sum'] = Total_pivot[ESMode_columns].sum(axis=1)\n",
    "\n",
    "# Combine features and labels\n",
    "train_data = pd.merge(energy_data,Total_pivot, on=['Time', 'BS'], how='left')\n",
    "test_data = pd.merge(power_consumption_prediction, Total_pivot, on=['Time', 'BS'], how='left')\n",
    "combined_data = pd.concat([train_data, test_data], ignore_index=False)\n",
    "\n",
    "# perform one-hot encoding on time and BS\n",
    "combined_data['Time'] = pd.to_datetime(combined_data['Time']).dt.dayofyear*24+pd.to_datetime(combined_data['Time']).dt.hour \n",
    "combined_data = pd.get_dummies(combined_data, columns=['Time','BS'])\n",
    "\n",
    "# improve generalization\n",
    "bs_columns = [col for col in combined_data.columns if col.startswith('BS')]\n",
    "for col in bs_columns:\n",
    "    combined_data[col] = combined_data[col] / 100 # Scale the selected columns by 100\n",
    "\n",
    "# Obtain training set and test set\n",
    "train_data = combined_data.iloc[:len(train_data)]\n",
    "test_data = combined_data.iloc[len(train_data):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete features with a standard deviation of 0\n",
    "def preprocess_data(df, columns_with_std_zero):\n",
    "    X = df.drop(columns=['Energy','w'] + columns_with_std_zero)\n",
    "    y = df['Energy']\n",
    "    return X, y\n",
    "\n",
    "columns_with_std_zero1 = train_data.columns[train_data.std() == 0].tolist()\n",
    "columns_with_std_zero2 = train_data.columns[test_data.std() == 0].tolist()\n",
    "columns_with_std_zero = list(set(columns_with_std_zero1 + columns_with_std_zero2))\n",
    "\n",
    "# divide training set and test set into features and labels\n",
    "X,y = preprocess_data(train_data,columns_with_std_zero)\n",
    "X_test,y_test = preprocess_data(test_data,columns_with_std_zero)\n",
    "print(X.columns)\n",
    "print(train_data.columns)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "def MAPELoss(output, target):\n",
    "    return torch.mean(torch.abs((target - output) / target))\n",
    "\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    y_true = y_true.cpu().detach().numpy()\n",
    "    y_pred = y_pred.cpu().detach().numpy()\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# Define Model\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.fc(x)\n",
    "        return x * y\n",
    "\n",
    "# class MyModel(nn.Module):\n",
    "#     def __init__(self, input_dim, num_layers=7):\n",
    "#         super(MyModel, self).__init__()\n",
    "        \n",
    "#         self.layers = nn.ModuleList()\n",
    "        \n",
    "#         self.layers.append(nn.Linear(input_dim, 1024))\n",
    "#         self.layers.append(SELayer(1024))\n",
    "        \n",
    "#         for i in range(num_layers - 1):\n",
    "#             input_dim = 1024 // (2 ** i)\n",
    "#             output_dim = 1024 // (2 ** (i + 1))\n",
    "#             self.layers.append(nn.Linear(input_dim, output_dim))\n",
    "#             self.layers.append(SELayer(output_dim))\n",
    "        \n",
    "#         input_dim = 1024 // (2 ** (num_layers - 1))\n",
    "#         self.layers.append(nn.Linear(input_dim, 1))\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         for layer in self.layers:\n",
    "#             if isinstance(layer, nn.Linear):\n",
    "#                 x = F.leaky_relu(layer(x))\n",
    "#             elif isinstance(layer, SELayer):\n",
    "#                 x = layer(x)\n",
    "#         return x\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_layers=7):\n",
    "        super(MyModel, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        self.layers.append(nn.Linear(input_dim, 1024))\n",
    "        self.layers.append(nn.BatchNorm1d(1024))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Dropout(0.5))\n",
    "        self.layers.append(SELayer(1024))\n",
    "\n",
    "        for i in range(num_layers - 1):\n",
    "            input_dim = 1024 // (2 ** i)\n",
    "            output_dim = 1024 // (2 ** (i + 1))\n",
    "            self.layers.append(nn.Linear(input_dim, output_dim))\n",
    "            self.layers.append(nn.BatchNorm1d(output_dim))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Dropout(0.5))\n",
    "            self.layers.append(SELayer(output_dim))\n",
    "\n",
    "        input_dim = 1024 // (2 ** (num_layers - 1))\n",
    "        self.layers.append(nn.Linear(input_dim, 1))\n",
    "        # self.layers.append(SELayer(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            # print(x.shape)\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                x = F.leaky_relu(layer(x))\n",
    "            elif isinstance(layer, SELayer):\n",
    "                x = layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def weight_init(m):\n",
    "    if isinstance(m, torch.nn.Conv1d):\n",
    "        torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "continue_train = 0\n",
    "X_tensor = torch.Tensor(X.values).to(device)\n",
    "y_tensor = torch.Tensor(y.values).unsqueeze(1).to(device)\n",
    "X_data = TensorDataset(X_tensor, y_tensor)\n",
    "X_loader = DataLoader(dataset=X_data, batch_size=2**11, shuffle=True)\n",
    "\n",
    "if continue_train == 1:\n",
    "    model = torch.load('./model_best_w5.pth')\n",
    "\n",
    "else:\n",
    "    model = MyModel(X.shape[1]).to(device)\n",
    "    # model.apply(weight_init)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = MAPELoss\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=35000, gamma=0.1)  \n",
    "\n",
    "# Train the model\n",
    "num_epochs = 35000\n",
    "\n",
    "best_mape = 100\n",
    "best_epoch = 0\n",
    "\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        running_mape = 0.0\n",
    "        model.train()  # Set the model to training mode\n",
    "        for i, data in enumerate(X_loader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # calculate running loss and MAPE\n",
    "            running_loss += loss.item()*100\n",
    "            # running_mape += calculate_mape(labels, outputs).item()\n",
    "\n",
    "        current_lr = scheduler.get_lr()\n",
    "        print('Epoch %d, loss: %.3f, learning rate: %.6f' % (epoch + 1, running_loss / len(X_loader), current_lr[0]))\n",
    "        scheduler.step()\n",
    "\n",
    "        if best_mape > (running_loss / len(X_loader)):\n",
    "            torch.save(model, \"./model_best_w5.pth\")\n",
    "            best_mape = running_loss / len(X_loader)\n",
    "            best_epoch = epoch + 1\n",
    "        if running_loss / len(X_loader) <= 1.2:\n",
    "            break\n",
    "    torch.save(model, \"./model_last_w5.pth\")\n",
    "    print('Finished Training')\n",
    "    print('best_epoch %d, best_mape %.3f' % (best_epoch, best_mape))\n",
    "except KeyboardInterrupt:\n",
    "    torch.save(model, \"./model_last_w5.pth\")\n",
    "    print(f\"Model saved to {'./model_last_w5.pth'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor = torch.Tensor(X_test.values).to(device)\n",
    "y_test_tensor = torch.Tensor(y_test.values).unsqueeze(1).to(device)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "evaluate_flag = 1\n",
    "if evaluate_flag == 1:\n",
    "    model = torch.load(\"BestModel/best_w5.pth\")\n",
    "else:\n",
    "    model = torch.load(\"./model_best_w5.pth\")\n",
    "    \n",
    "model.eval()  # Set the model to evaluation mode\n",
    "predictions = []\n",
    "with torch.no_grad():  # We don't need gradients for prediction\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        inputs, _ = data  # Ignore the weights\n",
    "        outputs = model(inputs)\n",
    "        predictions.extend(outputs.cpu().numpy())\n",
    "\n",
    "predictions_numpy = np.array(predictions)\n",
    "predictions_flatten = predictions_numpy.flatten()\n",
    "predictions_series = pd.Series(predictions_flatten, name=\"PredictedEnergy\")\n",
    "\n",
    "test_data_new = pd.read_csv('EnergyContest/power_consumption_prediction.csv')\n",
    "test_data_new['Energy'] = predictions_series\n",
    "test_data_new['ID'] = test_data_new['Time'].astype(str) + \"_\" + test_data_new['BS'].astype(str)\n",
    "test_data_new = test_data_new[['ID', 'Energy']]\n",
    "test_data_new.to_csv('./power_consumption_prediction_w5.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:netopt]",
   "language": "python",
   "name": "conda-env-netopt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
