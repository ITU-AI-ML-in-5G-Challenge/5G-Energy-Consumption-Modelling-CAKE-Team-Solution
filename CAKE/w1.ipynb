{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, columns_with_std_zero):\n",
    "    X = df.drop(columns=['Energy','w'] + columns_with_std_zero)\n",
    "    y = df['Energy']\n",
    "    return X, y\n",
    "\n",
    "# Define loss function\n",
    "def MAPELoss(output, target):\n",
    "    return torch.mean(torch.abs((target - output) / target))\n",
    "\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    y_true = y_true.cpu().detach().numpy()\n",
    "    y_pred = y_pred.cpu().detach().numpy()\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def calculate_wmape(y_true,y_pred, weights):\n",
    "    return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
    "\n",
    "def WMAPELoss(y_pred, y_true, weights):\n",
    "    return torch.sum(weights * torch.abs(y_true - y_pred)) / torch.sum(weights)\n",
    "\n",
    "# define model\n",
    "class LinearBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearBlock, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.bn = nn.BatchNorm1d(output_dim)\n",
    "        self.act = nn.LeakyReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "    \n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.block1 = LinearBlock(input_dim, 512)\n",
    "        # self.block2 = LinearBlock(1024, 512)\n",
    "        self.block3 = LinearBlock(512, 256)\n",
    "        self.block4 = LinearBlock(256, 128)\n",
    "        self.block5 = LinearBlock(128, 64)\n",
    "        self.fc_final = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        # x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = self.fc_final(x)\n",
    "        return x\n",
    "\n",
    "def weight_init(m):\n",
    "    if isinstance(m, torch.nn.Conv1d):\n",
    "        torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_info = pd.read_csv('EnergyContest/BSinfo.csv')\n",
    "cell_data = pd.read_csv('EnergyContest/CLdata.csv', parse_dates=['Time'])\n",
    "energy_data = pd.read_csv('EnergyContest/ECdata.csv', parse_dates=['Time'])\n",
    "power_consumption_prediction = pd.read_csv('EnergyContest/power_consumption_prediction.csv', parse_dates=['Time'])\n",
    "\n",
    "duplicate_rows = cell_data.duplicated(subset=['Time', 'BS'])\n",
    "print(\"Number of duplicate rows based on Time and BS:\", duplicate_rows.sum())\n",
    "cell_data = cell_data.drop_duplicates(subset=['Time', 'BS'])\n",
    "\n",
    "\n",
    "Total = pd.merge(base_info,cell_data, on=['BS', 'CellName'], how='inner')\n",
    "train_data = pd.merge(energy_data,Total, on=['Time', 'BS'], how='left')\n",
    "test_data = pd.merge(power_consumption_prediction, Total, on=['Time', 'BS'], how='left')\n",
    "\n",
    "combined_data = pd.concat([train_data, test_data], ignore_index=False)\n",
    "combined_data['Time'] = pd.to_datetime(combined_data['Time']).dt.dayofyear*24+pd.to_datetime(combined_data['Time']).dt.hour\n",
    "combined_data.drop(columns=['CellName'],inplace = True)\n",
    "combined_data = pd.get_dummies(combined_data, columns=['Mode','RUType','Antennas','Bandwidth','Frequency','Time','BS'])\n",
    "\n",
    "train_data = combined_data.iloc[:len(train_data)]\n",
    "test_data = combined_data.iloc[len(train_data):]\n",
    "\n",
    "columns_with_std_zero1 = train_data.columns[train_data.std() == 0].tolist()\n",
    "print(len(columns_with_std_zero1))\n",
    "print(columns_with_std_zero1)\n",
    "print(len(train_data.columns[train_data.mean() == 0].tolist()))\n",
    "columns_with_std_zero2 = train_data.columns[test_data.std() == 0].tolist()\n",
    "print(len(columns_with_std_zero2))\n",
    "print(columns_with_std_zero2)\n",
    "columns_with_std_zero = list(set(columns_with_std_zero1+ columns_with_std_zero2))\n",
    "print(len(columns_with_std_zero))\n",
    "\n",
    "X,y = preprocess_data(train_data,columns_with_std_zero)\n",
    "X_test,y_test = preprocess_data(test_data,columns_with_std_zero)\n",
    "\n",
    "print(columns_with_std_zero)\n",
    "print(X.shape[1])\n",
    "print(X.shape[0])\n",
    "print(X_test.shape[1])\n",
    "print(X_test.shape[0])\n",
    "print(Total.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "continue_train = 0\n",
    "X_tensor = torch.Tensor(X.values).to(device)\n",
    "y_tensor = torch.Tensor(y.values).unsqueeze(1).to(device)\n",
    "X_data = TensorDataset(X_tensor, y_tensor)\n",
    "X_loader = DataLoader(dataset=X_data, batch_size=512, shuffle=True)\n",
    "\n",
    "if continue_train == 1:\n",
    "    model = torch.load(\"./model_last_w1.pth\")\n",
    "else:\n",
    "    model = MyModel(X.shape[1]).to(device)\n",
    "\n",
    "criterion = MAPELoss\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10000\n",
    "\n",
    "best_mape = 100\n",
    "best_epoch = 0\n",
    "\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        running_mape = 0.0\n",
    "        model.train()  # Set the model to training mode\n",
    "        for i, data in enumerate(X_loader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # print(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # calculate running loss and MAPE\n",
    "            running_loss += loss.item()*100\n",
    "            # running_mape += calculate_mape(labels, outputs).item()\n",
    "        print('Epoch %d, loss: %.3f' % (epoch + 1, running_loss / len(X_loader)))\n",
    "        # print('Epoch %d, loss: %.3f, MAPE: %.3f' % (epoch + 1, running_loss / len(X_loader), running_mape / len(X_loader)))\n",
    "\n",
    "        if best_mape > (running_loss / len(X_loader)):\n",
    "            torch.save(model, \"./model_best_w1.pth\")\n",
    "            best_mape = running_loss / len(X_loader)\n",
    "            best_epoch = epoch + 1\n",
    "        if  running_loss / len(X_loader) <= 0.4:\n",
    "            break\n",
    "    torch.save(model, \"./model_last_w1.pth\")\n",
    "    print('Finished Training')\n",
    "except KeyboardInterrupt:\n",
    "    torch.save(model, \"./model_last_w1.pth\")\n",
    "    print(f\"Model saved to {'./model_last_w1.pth'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor = torch.Tensor(X_test.values).to(device)\n",
    "y_test_tensor = torch.Tensor(y_test.values).unsqueeze(1).to(device)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=1, shuffle=False)\n",
    "\n",
    "evaluate_flag = 1\n",
    "if evaluate_flag == 1:\n",
    "    model = torch.load(\"BestModel/best_w1.pth\")\n",
    "else:\n",
    "    model = torch.load(\"./model_best_w1.pth\")\n",
    "    \n",
    "model.eval()  # Set the model to evaluation mode\n",
    "predictions = []\n",
    "with torch.no_grad():  # We don't need gradients for prediction\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        inputs, _ = data  # Ignore the weights\n",
    "        outputs = model(inputs)\n",
    "        predictions.extend(outputs.cpu().numpy())\n",
    "\n",
    "predictions_numpy = np.array(predictions)\n",
    "predictions_flatten = predictions_numpy.flatten()\n",
    "predictions_series = pd.Series(predictions_flatten, name=\"PredictedEnergy\")\n",
    "\n",
    "test_data_new = pd.read_csv('EnergyContest/power_consumption_prediction.csv')\n",
    "test_data_new['Energy'] = predictions_series\n",
    "test_data_new['ID'] = test_data_new['Time'].astype(str) + \"_\" + test_data_new['BS'].astype(str)\n",
    "test_data_new = test_data_new[['ID', 'Energy']]\n",
    "test_data_new.to_csv('./power_consumption_prediction_w1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:netopt]",
   "language": "python",
   "name": "conda-env-netopt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
